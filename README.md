# ResearchBucket

## Research Projects

### Local LLM Agents Research

Comprehensive research on running LLM agents locally for coding assistance instead of using cloud resources.

**Topics Covered:**
- Agent-specific setup guides (Claude, Gemini, Copilot, GPT-5.x)
- Deployment models (Local-only, Single-project, Hybrid, Networked)
- Infrastructure (MCP servers, Security, Hardware, Containers)
- Comprehensive comparisons and analysis
- Use-case specific guides (Unity, C#, Web, React, Documentation)

**Quick Links:**
- [ðŸ“– Full Research Documentation](./LocalLLMAgents/README.md)
- [ðŸ“Š Executive Summary](./LocalLLMAgents/SUMMARY.md)
- [âš¡ Quick Start Guide](./LocalLLMAgents/README.md#getting-started)

**Key Findings:**
- RTX 4070 with 12GB VRAM is ideal for running 13B-33B models
- Hybrid deployment (local-first with cloud fallback) provides best balance
- 85-95% of cloud quality achievable locally for most coding tasks
- Breaks even vs cloud-only at 12 months, saves $2400-3700 over 3 years
- Complete privacy and data sovereignty

**Hardware Tested:**
- Windows PC with NVIDIA RTX 4070 (12GB VRAM)
- M2 Mac with unified memory (16-64GB)

**Branch:** `research/local-llm-agents`

---

## About

This repository contains research and experiments for various technology topics.